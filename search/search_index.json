{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Documentation Quick Start Tutorials API Reference Manual For Developers","title":"Documentation"},{"location":"#documentation","text":"","title":"Documentation"},{"location":"#quick-start-tutorials","text":"","title":"Quick Start Tutorials"},{"location":"#api-reference-manual","text":"","title":"API Reference Manual"},{"location":"#for-developers","text":"","title":"For Developers"},{"location":"get_started/","text":"Get Started An executable configuration graph. Note: We describe the concept of this core module in following few lines and show some pesudo-codes. This is very close to but not the same as the real code. An acyclic directed hypergraph G consists of a set of vertices V and a set of hyperarcs H , where a hyperarc is a pair <X, Y> , X and Y non empty subset of V . We have a tag system that split the vertices V into maybe overlapping subsets V_i , that each of which is a degenerated hypergraph G_i that only consists of vertices V_i and a set of hyperarcs H_i so that each hyperarc is a pair <x, Y> , where x \\in V_i and Y \\subset V_i . We call tails x as producers and heads Y as consumers in each hyperarc, this states the dependencies. User defines a vertice ( Node in the code) by specify a computation process f ( forward in the code) and the resources R ( Dataset s, nn.Module s, imperatively programmed function definitions such as losses and metrics, etc.) needed by it. vertice_1 = Node ( name = \"consumer_node_name\" , resources = ... , forward = lambda n , x : do_something_with ( n . resources , x [ \"producer_node_name\" ]), tags = [ \"group1\" , \"group2\" ], ) A longer version of forward parameter that corresponds to the previous notation would be forward = lambda self, V_i: do_something_with(self.resources, V_i[\"x\"]) , but we will stick to the shorter version in the code. So at the time of configuration, we are able to define every material as a node, and the name of nodes can be duplicated, i.e. multiple x\\in V can have the same identifier, as long as they does not have the same tag i that selects V_i . The tags mechanism is flexible. Every node can have multiple of them, and multiple tags can be specified so that a union of subsets will be retrieved. If no tag is specified for a node, a default tag * will be used and a retrival will always include the * group. ```python hyper_graph = HyperGraph([ vertice_1, vertice_2, ..., vertice_n, ]) activated_graph = hyper_graph[\"group1\", \"group3\", \"group5\"] freeze_and_execute(activated_graph)","title":"Get Started"},{"location":"get_started/#get-started","text":"An executable configuration graph. Note: We describe the concept of this core module in following few lines and show some pesudo-codes. This is very close to but not the same as the real code. An acyclic directed hypergraph G consists of a set of vertices V and a set of hyperarcs H , where a hyperarc is a pair <X, Y> , X and Y non empty subset of V . We have a tag system that split the vertices V into maybe overlapping subsets V_i , that each of which is a degenerated hypergraph G_i that only consists of vertices V_i and a set of hyperarcs H_i so that each hyperarc is a pair <x, Y> , where x \\in V_i and Y \\subset V_i . We call tails x as producers and heads Y as consumers in each hyperarc, this states the dependencies. User defines a vertice ( Node in the code) by specify a computation process f ( forward in the code) and the resources R ( Dataset s, nn.Module s, imperatively programmed function definitions such as losses and metrics, etc.) needed by it. vertice_1 = Node ( name = \"consumer_node_name\" , resources = ... , forward = lambda n , x : do_something_with ( n . resources , x [ \"producer_node_name\" ]), tags = [ \"group1\" , \"group2\" ], ) A longer version of forward parameter that corresponds to the previous notation would be forward = lambda self, V_i: do_something_with(self.resources, V_i[\"x\"]) , but we will stick to the shorter version in the code. So at the time of configuration, we are able to define every material as a node, and the name of nodes can be duplicated, i.e. multiple x\\in V can have the same identifier, as long as they does not have the same tag i that selects V_i . The tags mechanism is flexible. Every node can have multiple of them, and multiple tags can be specified so that a union of subsets will be retrieved. If no tag is specified for a node, a default tag * will be used and a retrival will always include the * group. ```python hyper_graph = HyperGraph([ vertice_1, vertice_2, ..., vertice_n, ]) activated_graph = hyper_graph[\"group1\", \"group3\", \"group5\"] freeze_and_execute(activated_graph)","title":"Get Started"},{"location":"documents/","text":"API Overview Modules api.scripts.wait_process : wait for a process to finish. core.graph : contains Node and ExecutableGraph . core.hypergraph llutil llutil.argparser : parse arguments for functions and command line. llutil.collections llutil.config llutil.dictprocess llutil.launcher llutil.logging : logging utilities. llutil.multiprocessing : a drop-in replacement for torch.multiprocessing . llutil.pycuda : Integrates PyCUDA to PyTorch and ice. llutil.test : helps developers of ice to test. Classes graph.ExecutableGraph graph.GraphOutputCache graph.InvalidURIError : An Exception raised when valid node URI is expected. graph.Node : This class defines the executable node. graph.StopTask : An Exception raised to exit current task. hypergraph.HyperGraph : HyperGraph is the container for all nodes. hypergraph.Repeat hypergraph.Task argparser.ArgumentMissingError : Raised when a required argument is missing from command line. argparser.ArgumentTypeError : Raised when converting an argument failed. argparser.FlexibleArgParser : A flexible and lightweight argument parser that saves loads of code. collections.ConfigDict : stores multi-level configurations easily. collections.Counter : count values by group. collections.Dict : access dict values as attributes. config.Configurable launcher.ElasticLauncher : A helper Configurable class for torchrun and torch.distributed.launch . pycuda.CUDAModule : Just-In-Time compilation of a set of CUDA kernel functions and device functions from source. Functions argparser.as_dict : helps to regularize input into a dict. argparser.as_list : helps to regularize input into list of element. argparser.isa : an alias for python built-in isinstance . config.clone : clone configurables, containers, and ordinary objects recursively. config.configurable : This decorator delays the initialization of cls until freeze() . config.freeze : freeze configurables recursively. config.has_builder config.is_configurable : check if a class or an object is configurable. config.make_configurable : This function converts multiple existing classes to configurables. config.objattr dictprocess.Collect : a predefined DictProcessor that keep only selected entries. dictprocess.Compose : a predefined DictProcessor that composes a list of other DictProcessors together. dictprocess.dictprocess : a decorator that convert function into a DictProcessor ( Callable[[Dict], Dict] ). logging.get_logger : set up a simple logger that writes into stderr. multiprocessing.called_from_main : Another version of if __name__ == \"__main__\" that works everywhere. test.requires_n_gpus","title":"Index"},{"location":"documents/#api-overview","text":"","title":"API Overview"},{"location":"documents/#modules","text":"api.scripts.wait_process : wait for a process to finish. core.graph : contains Node and ExecutableGraph . core.hypergraph llutil llutil.argparser : parse arguments for functions and command line. llutil.collections llutil.config llutil.dictprocess llutil.launcher llutil.logging : logging utilities. llutil.multiprocessing : a drop-in replacement for torch.multiprocessing . llutil.pycuda : Integrates PyCUDA to PyTorch and ice. llutil.test : helps developers of ice to test.","title":"Modules"},{"location":"documents/#classes","text":"graph.ExecutableGraph graph.GraphOutputCache graph.InvalidURIError : An Exception raised when valid node URI is expected. graph.Node : This class defines the executable node. graph.StopTask : An Exception raised to exit current task. hypergraph.HyperGraph : HyperGraph is the container for all nodes. hypergraph.Repeat hypergraph.Task argparser.ArgumentMissingError : Raised when a required argument is missing from command line. argparser.ArgumentTypeError : Raised when converting an argument failed. argparser.FlexibleArgParser : A flexible and lightweight argument parser that saves loads of code. collections.ConfigDict : stores multi-level configurations easily. collections.Counter : count values by group. collections.Dict : access dict values as attributes. config.Configurable launcher.ElasticLauncher : A helper Configurable class for torchrun and torch.distributed.launch . pycuda.CUDAModule : Just-In-Time compilation of a set of CUDA kernel functions and device functions from source.","title":"Classes"},{"location":"documents/#functions","text":"argparser.as_dict : helps to regularize input into a dict. argparser.as_list : helps to regularize input into list of element. argparser.isa : an alias for python built-in isinstance . config.clone : clone configurables, containers, and ordinary objects recursively. config.configurable : This decorator delays the initialization of cls until freeze() . config.freeze : freeze configurables recursively. config.has_builder config.is_configurable : check if a class or an object is configurable. config.make_configurable : This function converts multiple existing classes to configurables. config.objattr dictprocess.Collect : a predefined DictProcessor that keep only selected entries. dictprocess.Compose : a predefined DictProcessor that composes a list of other DictProcessors together. dictprocess.dictprocess : a decorator that convert function into a DictProcessor ( Callable[[Dict], Dict] ). logging.get_logger : set up a simple logger that writes into stderr. multiprocessing.called_from_main : Another version of if __name__ == \"__main__\" that works everywhere. test.requires_n_gpus","title":"Functions"},{"location":"documents/api.scripts.wait_process/","text":"module api.scripts.wait_process wait for a process to finish. usage: wait_process [-h] PIDS [PIDS ...] This command just blocks until all processes specified in PIDS exits. positional arguments: PIDS optional arguments: -h, --help show this help message and exit","title":"Api.scripts.wait process"},{"location":"documents/api.scripts.wait_process/#module-apiscriptswait_process","text":"wait for a process to finish. usage: wait_process [-h] PIDS [PIDS ...] This command just blocks until all processes specified in PIDS exits. positional arguments: PIDS optional arguments: -h, --help show this help message and exit","title":"module api.scripts.wait_process"},{"location":"documents/api.scripts.waitfinish/","text":"module api.scripts.waitfinish usage: waitfinish [-h] PIDS [PIDS ...] This command just blocks until all processes specified in PIDS exits. positional arguments: PIDS optional arguments: -h, --help show this help message and exit","title":"Api.scripts.waitfinish"},{"location":"documents/api.scripts.waitfinish/#module-apiscriptswaitfinish","text":"usage: waitfinish [-h] PIDS [PIDS ...] This command just blocks until all processes specified in PIDS exits. positional arguments: PIDS optional arguments: -h, --help show this help message and exit","title":"module api.scripts.waitfinish"},{"location":"documents/core.graph/","text":"module core.graph contains Node and ExecutableGraph . class InvalidURIError An Exception raised when valid node URI is expected. class StopTask An Exception raised to exit current task. class Node This class defines the executable node. A executable graph is defined by a collection of executable nodes and their dependency relationships. A node is executable if it has at least following phases of execution: forward , backward , update . Different subclass of nodes may implement them differently. This class is designed to be executed easily in batch mode (see ExecutableGraph.apply() for details), so that a bunch of nodes can execute together, respecting several synchronization points between phases. The dependency relationship is determined at runtime by how user access the graph argument of Node.forward() function. The graph argument is actually a cache (a GraphOutputCache instance) of the graph nodes outputs. The results of precedent nodes will be saved in the cache, so dependents can retrieve them easily. method __freeze__ __freeze__ ( forward : Callable [[ ForwardRef ( 'Node' ), ForwardRef ( 'GraphOutputCache' )], Any ] = None , ** resources ) \u2192 None initialize the node. Args: forward (Callable[[self, x: GraphOutputCache ], Any], optional): if specified, will override the original forward method. **resources : resources will be updated into the attributes of Node. property device the assigned device by current launcher. property name the node name in the current activated ExecutableGraph . property step_mode whether current task is running by step (True) or by epoch (False). property training whether current task is training. property uris the node URIs <tag/name> in the current HyperGraph . method backward backward () calculates gradients. method clean_up clean_up () an event hook for clean up all resources at switching executable graphs, e.g. clear device memory, closing files, etc. method dry_run dry_run () only update states about progress. method epoch_end epoch_end () an event hook for epoch end. (only for epoch mode) method epoch_start epoch_start () an event hook for epoch start. (only for epoch mode) method forward forward ( graph : 'GraphOutputCache' ) calculates forward pass results of the node, inputs of current executable graph can be directly retrieved from graph argument. method load_state_dict load_state_dict ( state_dict ) resumes node state from state_dict. method prepare prepare () an event hook for prepare all resources at switching executable graphs, e.g. moving models to device, initialize dataloaders, etc. method state_dict state_dict () returns serialization of current node. method update update () update parameters or buffers, e.g. using SGD based optimizer to update parameters. class GraphOutputCache method __init__ __init__ ( graph : 'ExecutableGraph' ) \u2192 None method __getitem__ __getitem__ ( name ) Execute node with name name if not executed, return the last executed cache else. method clear clear () Clear the cache, next calls to __getitem__ will recalculate. class ExecutableGraph method __init__ __init__ () \u2192 None method add_node add_node ( node_name , node , group_names ) method apply apply ( method : str , * args , filter : Callable [[ Node ], bool ] = lambda _ : True ,, ** kwds ) method clean_up_nodes clean_up_nodes () method iterate iterate ( hyper_graph ) method prepare_nodes prepare_nodes ()","title":"Core.graph"},{"location":"documents/core.graph/#module-coregraph","text":"contains Node and ExecutableGraph .","title":"module core.graph"},{"location":"documents/core.graph/#class-invalidurierror","text":"An Exception raised when valid node URI is expected.","title":"class InvalidURIError"},{"location":"documents/core.graph/#class-stoptask","text":"An Exception raised to exit current task.","title":"class StopTask"},{"location":"documents/core.graph/#class-node","text":"This class defines the executable node. A executable graph is defined by a collection of executable nodes and their dependency relationships. A node is executable if it has at least following phases of execution: forward , backward , update . Different subclass of nodes may implement them differently. This class is designed to be executed easily in batch mode (see ExecutableGraph.apply() for details), so that a bunch of nodes can execute together, respecting several synchronization points between phases. The dependency relationship is determined at runtime by how user access the graph argument of Node.forward() function. The graph argument is actually a cache (a GraphOutputCache instance) of the graph nodes outputs. The results of precedent nodes will be saved in the cache, so dependents can retrieve them easily.","title":"class Node"},{"location":"documents/core.graph/#method-__freeze__","text":"__freeze__ ( forward : Callable [[ ForwardRef ( 'Node' ), ForwardRef ( 'GraphOutputCache' )], Any ] = None , ** resources ) \u2192 None initialize the node. Args: forward (Callable[[self, x: GraphOutputCache ], Any], optional): if specified, will override the original forward method. **resources : resources will be updated into the attributes of Node.","title":"method __freeze__"},{"location":"documents/core.graph/#property-device","text":"the assigned device by current launcher.","title":"property device"},{"location":"documents/core.graph/#property-name","text":"the node name in the current activated ExecutableGraph .","title":"property name"},{"location":"documents/core.graph/#property-step_mode","text":"whether current task is running by step (True) or by epoch (False).","title":"property step_mode"},{"location":"documents/core.graph/#property-training","text":"whether current task is training.","title":"property training"},{"location":"documents/core.graph/#property-uris","text":"the node URIs <tag/name> in the current HyperGraph .","title":"property uris"},{"location":"documents/core.graph/#method-backward","text":"backward () calculates gradients.","title":"method backward"},{"location":"documents/core.graph/#method-clean_up","text":"clean_up () an event hook for clean up all resources at switching executable graphs, e.g. clear device memory, closing files, etc.","title":"method clean_up"},{"location":"documents/core.graph/#method-dry_run","text":"dry_run () only update states about progress.","title":"method dry_run"},{"location":"documents/core.graph/#method-epoch_end","text":"epoch_end () an event hook for epoch end. (only for epoch mode)","title":"method epoch_end"},{"location":"documents/core.graph/#method-epoch_start","text":"epoch_start () an event hook for epoch start. (only for epoch mode)","title":"method epoch_start"},{"location":"documents/core.graph/#method-forward","text":"forward ( graph : 'GraphOutputCache' ) calculates forward pass results of the node, inputs of current executable graph can be directly retrieved from graph argument.","title":"method forward"},{"location":"documents/core.graph/#method-load_state_dict","text":"load_state_dict ( state_dict ) resumes node state from state_dict.","title":"method load_state_dict"},{"location":"documents/core.graph/#method-prepare","text":"prepare () an event hook for prepare all resources at switching executable graphs, e.g. moving models to device, initialize dataloaders, etc.","title":"method prepare"},{"location":"documents/core.graph/#method-state_dict","text":"state_dict () returns serialization of current node.","title":"method state_dict"},{"location":"documents/core.graph/#method-update","text":"update () update parameters or buffers, e.g. using SGD based optimizer to update parameters.","title":"method update"},{"location":"documents/core.graph/#class-graphoutputcache","text":"","title":"class GraphOutputCache"},{"location":"documents/core.graph/#method-__init__","text":"__init__ ( graph : 'ExecutableGraph' ) \u2192 None","title":"method __init__"},{"location":"documents/core.graph/#method-__getitem__","text":"__getitem__ ( name ) Execute node with name name if not executed, return the last executed cache else.","title":"method __getitem__"},{"location":"documents/core.graph/#method-clear","text":"clear () Clear the cache, next calls to __getitem__ will recalculate.","title":"method clear"},{"location":"documents/core.graph/#class-executablegraph","text":"","title":"class ExecutableGraph"},{"location":"documents/core.graph/#method-__init___1","text":"__init__ () \u2192 None","title":"method __init__"},{"location":"documents/core.graph/#method-add_node","text":"add_node ( node_name , node , group_names )","title":"method add_node"},{"location":"documents/core.graph/#method-apply","text":"apply ( method : str , * args , filter : Callable [[ Node ], bool ] = lambda _ : True ,, ** kwds )","title":"method apply"},{"location":"documents/core.graph/#method-clean_up_nodes","text":"clean_up_nodes ()","title":"method clean_up_nodes"},{"location":"documents/core.graph/#method-iterate","text":"iterate ( hyper_graph )","title":"method iterate"},{"location":"documents/core.graph/#method-prepare_nodes","text":"prepare_nodes ()","title":"method prepare_nodes"},{"location":"documents/core.hypergraph/","text":"module core.hypergraph class Task method __init__ __init__ ( train : bool , tags = '*' , ** kwds ) class Repeat method __init__ __init__ ( tasks : List [ _Task ], times : int ) \u2192 None class HyperGraph HyperGraph is the container for all nodes. method __init__ __init__ () \u2192 None method add_node add_node ( name , node , tags = '*' ) method run run ( self , tasks , device = 'auto' ) Ellipsis method run run ( self , tasks , launcher : ElasticLauncher = 'auto' ) Ellipsis","title":"Core.hypergraph"},{"location":"documents/core.hypergraph/#module-corehypergraph","text":"","title":"module core.hypergraph"},{"location":"documents/core.hypergraph/#class-task","text":"","title":"class Task"},{"location":"documents/core.hypergraph/#method-__init__","text":"__init__ ( train : bool , tags = '*' , ** kwds )","title":"method __init__"},{"location":"documents/core.hypergraph/#class-repeat","text":"","title":"class Repeat"},{"location":"documents/core.hypergraph/#method-__init___1","text":"__init__ ( tasks : List [ _Task ], times : int ) \u2192 None","title":"method __init__"},{"location":"documents/core.hypergraph/#class-hypergraph","text":"HyperGraph is the container for all nodes.","title":"class HyperGraph"},{"location":"documents/core.hypergraph/#method-__init___2","text":"__init__ () \u2192 None","title":"method __init__"},{"location":"documents/core.hypergraph/#method-add_node","text":"add_node ( name , node , tags = '*' )","title":"method add_node"},{"location":"documents/core.hypergraph/#method-run","text":"run ( self , tasks , device = 'auto' ) Ellipsis","title":"method run"},{"location":"documents/core.hypergraph/#method-run_1","text":"run ( self , tasks , launcher : ElasticLauncher = 'auto' ) Ellipsis","title":"method run"},{"location":"documents/llutil.argparser/","text":"module llutil.argparser parse arguments for functions and command line. This module provides helper functions for commonly used argument processing for functions, and a FlexibleArgParser for command line argument parsing. The default singleton of this argument parser is accessable via ice.args . Global Variables REQUIRED function isa isa ( obj , types ) an alias for python built-in isinstance . function as_list as_list ( maybe_element ) helps to regularize input into list of element. No matter what is input, will output a list for your iteration. Basic Examples: assert as_list ( \"string\" ) == [ \"string\" ] assert as_list ([ \"string\" , \"string\" ]) == [ \"string\" , \"string\" ] assert as_list (( \"string\" , \"string\" )) == [ \"string\" , \"string\" ] assert as_list ([[ \"string\" , \"string\" ]]) == [ \"string\" , \"string\" ] An Application Example: def func ( * args ): return as_list ( args ) assert func ( \"a\" , \"b\" ) == [ \"a\" , \"b\" ] assert func ([ \"a\" , \"b\" ]) == [ \"a\" , \"b\" ] function as_dict as_dict ( maybe_element , key ) helps to regularize input into a dict. if maybe_element is not a dict, will return a dict with single key as {key:maybe_element} , else will return maybe_element . Args: maybe_element : a dict or any object. key : the sole key. Returns: dict : ensures to be a dict. Example: assert as_dict ({ \"k\" : \"v\" }, \"k\" ) == { \"k\" : \"v\" } assert as_dict ( \"v\" , \"k\" ) == { \"k\" : \"v\" } class ArgumentMissingError Raised when a required argument is missing from command line. class ArgumentTypeError Raised when converting an argument failed. class FlexibleArgParser A flexible and lightweight argument parser that saves loads of code. This module works differently compared to python built-in argparse module. It accepts two types of command line arguments, i.e. positional and keyword based (options). The keyword based arguments (options) should be specified as key=value or key=\"value\" . The positional arguments is indexed directly using an integer, but this feature is not recommended. Example: import ice # same as `python <script>.py 2 k1=4` in shell. ice . args . parse_args ([ \"2\" , \"k1=4\" ]) # setdefault() generally is optional. ice . args . setdefault ( \"k1\" , 8 , int ) ice . args . setdefault ( \"k2\" , 8 ) assert len ( ice . args ) == 3 assert 2 == int ( ice . args [ 0 ]) # default type is str. assert 4 == ice . args [ \"k1\" ] # as setdefault specified a type, here a conversion is not needed. assert 4 == ice . args . k1 # attribute also works. assert 8 == ice . args . k2 # use default value. ice . args [ \"k1\" ] = 1 ice . args . k3 = 1 ice . args . update ( k2 = 0 ) ice . args . update ({ 0 : - 1 }) assert - 1 == ice . args [ 0 ] assert 1 == ice . args [ \"k3\" ] assert 0 == ice . args . k2 Note: If you manually call parse_args() , call it before setdefault() . method __init__ __init__ () \u2192 None method parse_args parse_args ( argv ) Manually parse args. Args: argv (List[str]): simillar to sys.argv[1:] . method setdefault setdefault ( key , default , type = None , help = '' ) Set argument value under key as value , only if original entry does not exists. Args: key (int|str): the keyword. value : default_value to be set when orginal entry does not exists. Returns: original or updated value. method update update ( _FlexibleArgParser__dict = {}, ** kwds ) simillar to dict.update().","title":"Llutil.argparser"},{"location":"documents/llutil.argparser/#module-llutilargparser","text":"parse arguments for functions and command line. This module provides helper functions for commonly used argument processing for functions, and a FlexibleArgParser for command line argument parsing. The default singleton of this argument parser is accessable via ice.args .","title":"module llutil.argparser"},{"location":"documents/llutil.argparser/#global-variables","text":"REQUIRED","title":"Global Variables"},{"location":"documents/llutil.argparser/#function-isa","text":"isa ( obj , types ) an alias for python built-in isinstance .","title":"function isa"},{"location":"documents/llutil.argparser/#function-as_list","text":"as_list ( maybe_element ) helps to regularize input into list of element. No matter what is input, will output a list for your iteration. Basic Examples: assert as_list ( \"string\" ) == [ \"string\" ] assert as_list ([ \"string\" , \"string\" ]) == [ \"string\" , \"string\" ] assert as_list (( \"string\" , \"string\" )) == [ \"string\" , \"string\" ] assert as_list ([[ \"string\" , \"string\" ]]) == [ \"string\" , \"string\" ] An Application Example: def func ( * args ): return as_list ( args ) assert func ( \"a\" , \"b\" ) == [ \"a\" , \"b\" ] assert func ([ \"a\" , \"b\" ]) == [ \"a\" , \"b\" ]","title":"function as_list"},{"location":"documents/llutil.argparser/#function-as_dict","text":"as_dict ( maybe_element , key ) helps to regularize input into a dict. if maybe_element is not a dict, will return a dict with single key as {key:maybe_element} , else will return maybe_element . Args: maybe_element : a dict or any object. key : the sole key. Returns: dict : ensures to be a dict. Example: assert as_dict ({ \"k\" : \"v\" }, \"k\" ) == { \"k\" : \"v\" } assert as_dict ( \"v\" , \"k\" ) == { \"k\" : \"v\" }","title":"function as_dict"},{"location":"documents/llutil.argparser/#class-argumentmissingerror","text":"Raised when a required argument is missing from command line.","title":"class ArgumentMissingError"},{"location":"documents/llutil.argparser/#class-argumenttypeerror","text":"Raised when converting an argument failed.","title":"class ArgumentTypeError"},{"location":"documents/llutil.argparser/#class-flexibleargparser","text":"A flexible and lightweight argument parser that saves loads of code. This module works differently compared to python built-in argparse module. It accepts two types of command line arguments, i.e. positional and keyword based (options). The keyword based arguments (options) should be specified as key=value or key=\"value\" . The positional arguments is indexed directly using an integer, but this feature is not recommended. Example: import ice # same as `python <script>.py 2 k1=4` in shell. ice . args . parse_args ([ \"2\" , \"k1=4\" ]) # setdefault() generally is optional. ice . args . setdefault ( \"k1\" , 8 , int ) ice . args . setdefault ( \"k2\" , 8 ) assert len ( ice . args ) == 3 assert 2 == int ( ice . args [ 0 ]) # default type is str. assert 4 == ice . args [ \"k1\" ] # as setdefault specified a type, here a conversion is not needed. assert 4 == ice . args . k1 # attribute also works. assert 8 == ice . args . k2 # use default value. ice . args [ \"k1\" ] = 1 ice . args . k3 = 1 ice . args . update ( k2 = 0 ) ice . args . update ({ 0 : - 1 }) assert - 1 == ice . args [ 0 ] assert 1 == ice . args [ \"k3\" ] assert 0 == ice . args . k2 Note: If you manually call parse_args() , call it before setdefault() .","title":"class FlexibleArgParser"},{"location":"documents/llutil.argparser/#method-__init__","text":"__init__ () \u2192 None","title":"method __init__"},{"location":"documents/llutil.argparser/#method-parse_args","text":"parse_args ( argv ) Manually parse args. Args: argv (List[str]): simillar to sys.argv[1:] .","title":"method parse_args"},{"location":"documents/llutil.argparser/#method-setdefault","text":"setdefault ( key , default , type = None , help = '' ) Set argument value under key as value , only if original entry does not exists. Args: key (int|str): the keyword. value : default_value to be set when orginal entry does not exists. Returns: original or updated value.","title":"method setdefault"},{"location":"documents/llutil.argparser/#method-update","text":"update ( _FlexibleArgParser__dict = {}, ** kwds ) simillar to dict.update().","title":"method update"},{"location":"documents/llutil.collections/","text":"module llutil.collections class Dict access dict values as attributes. class Counter count values by group. Features: Get or set values using dictionary or attribute interface. Returns a zero count for missing items instead of raising a KeyError. a total() function that sums all values. Example: import ice cnt = ice . Counter () assert 0 == cnt [ 'x' ] assert 0 == cnt . x cnt . x += 1 assert 1 == cnt [ 'x' ] assert 1 == cnt . x cnt [ 'y' ] += 1 assert 2 == cnt . total () method total total () class ConfigDict stores multi-level configurations easily. Features: Get or set values using dictionary or attribute interface. Create empty dict for intermediate items instead of raising a KeyError. Example: import ice _C = ice . ConfigDict () _C . PROPERTY1 = 1 _C . GROUP1 . PROPERTY1 = 2","title":"Llutil.collections"},{"location":"documents/llutil.collections/#module-llutilcollections","text":"","title":"module llutil.collections"},{"location":"documents/llutil.collections/#class-dict","text":"access dict values as attributes.","title":"class Dict"},{"location":"documents/llutil.collections/#class-counter","text":"count values by group. Features: Get or set values using dictionary or attribute interface. Returns a zero count for missing items instead of raising a KeyError. a total() function that sums all values. Example: import ice cnt = ice . Counter () assert 0 == cnt [ 'x' ] assert 0 == cnt . x cnt . x += 1 assert 1 == cnt [ 'x' ] assert 1 == cnt . x cnt [ 'y' ] += 1 assert 2 == cnt . total ()","title":"class Counter"},{"location":"documents/llutil.collections/#method-total","text":"total ()","title":"method total"},{"location":"documents/llutil.collections/#class-configdict","text":"stores multi-level configurations easily. Features: Get or set values using dictionary or attribute interface. Create empty dict for intermediate items instead of raising a KeyError. Example: import ice _C = ice . ConfigDict () _C . PROPERTY1 = 1 _C . GROUP1 . PROPERTY1 = 2","title":"class ConfigDict"},{"location":"documents/llutil.config/","text":"module llutil.config function configurable configurable ( cls ) This decorator delays the initialization of cls until freeze() . Returns: decorated class which is now configurable. Example: import ice @ice . configurable class AClass : def __init__ ( self , a , b , c , d ): self . a = a self . b = b self . c = c self . d = d # partial initialization. i = AClass ( b = 0 ) # alter positional and keyword arguments afterwards. i [ 0 ] = 2 i [ 'b' ] = 1 i . update ({ 'c' : 3 , 'd' : 4 }) i . update ( d = 5 ) # unfrozen configurable can be printed as a legal construction python statement. assert repr ( i ) == \"AClass(a=2, b=1, c=3, d=5)\" # real initialization of original object. i . freeze () assert i . a == 2 and i . b == 1 function is_configurable is_configurable ( cls ) \u2192 bool check if a class or an object is configurable. Returns: bool Example: import ice import torch.nn as nn ice . make_configurable ( nn . Conv2d , nn . Linear ) assert ice . is_configurable ( nn . Conv2d ) function has_builder has_builder ( obj ) \u2192 bool function make_configurable make_configurable ( * classes ) This function converts multiple existing classes to configurables. Note: This have exactly the same effects of decorate each class with @configurable when defining the class. Each class only need to be decorated once, extra calling of conversion is ignored and has no side effects. Example: import ice import torch.nn as nn ice . make_configurable ( nn . Conv2d , nn . Linear ) assert ice . is_configurable ( nn . Conv2d ) function clone clone ( obj , deepcopy = True ) clone configurables, containers, and ordinary objects recursively. Args: obj (configurable or list/dict of configurables): the configurable object to be cloned. deepcopy (bool, optional): copy resources by value. Defaults to True. Returns: Unfrozen copy of the original configurable. import ice import torch.nn as nn ice . make_configurable ( nn . Conv2d , nn . Linear ) convcfg = nn . Conv2d ( 16 , 8 ) conv1x1 = convcfg . clone () # or ice.clone(convcfg) conv1x1 [ 'kernel_size' ] = 1 conv1x1 . freeze () # or ice.freeze(conv1x1) assert conv1x1 . kernel_size == ( 1 , 1 ) conv3x3 = convcfg . clone () conv3x3 [ 'kernel_size' ] = 3 conv3x3 . freeze () assert conv3x3 . kernel_size == ( 3 , 3 ) function freeze freeze ( obj , deepcopy = True ) freeze configurables recursively. Freezing is the process of building the configuration into real objects. Original __init__() functions of configurable classes declared by configurable or make_configurable now will be called recursively to initialize the real instance, also known as the frozen version of a configurable. Args: obj (configurable or list/dict of configurables): the configurable object to be freeze. deepcopy (bool, optional): copy resources by value. Defaults to True. Returns: Frozen version of the original configurable. Note: Freezing happens in-place, ignoring the returned value is safe. If a user wants to reuse the configuration feature, he can clone() the object before or after frozen with the same effect. Example: See examples for configurable and clone . function objattr objattr ( obj , attrname ) class Configurable method __init__ __init__ ( * args , ** kwds ) \u2192 None method clone clone ( deepcopy = True ) method freeze freeze ( deepcopy = True ) method update update ( explicit = {}, ** implicit )","title":"Llutil.config"},{"location":"documents/llutil.config/#module-llutilconfig","text":"","title":"module llutil.config"},{"location":"documents/llutil.config/#function-configurable","text":"configurable ( cls ) This decorator delays the initialization of cls until freeze() . Returns: decorated class which is now configurable. Example: import ice @ice . configurable class AClass : def __init__ ( self , a , b , c , d ): self . a = a self . b = b self . c = c self . d = d # partial initialization. i = AClass ( b = 0 ) # alter positional and keyword arguments afterwards. i [ 0 ] = 2 i [ 'b' ] = 1 i . update ({ 'c' : 3 , 'd' : 4 }) i . update ( d = 5 ) # unfrozen configurable can be printed as a legal construction python statement. assert repr ( i ) == \"AClass(a=2, b=1, c=3, d=5)\" # real initialization of original object. i . freeze () assert i . a == 2 and i . b == 1","title":"function configurable"},{"location":"documents/llutil.config/#function-is_configurable","text":"is_configurable ( cls ) \u2192 bool check if a class or an object is configurable. Returns: bool Example: import ice import torch.nn as nn ice . make_configurable ( nn . Conv2d , nn . Linear ) assert ice . is_configurable ( nn . Conv2d )","title":"function is_configurable"},{"location":"documents/llutil.config/#function-has_builder","text":"has_builder ( obj ) \u2192 bool","title":"function has_builder"},{"location":"documents/llutil.config/#function-make_configurable","text":"make_configurable ( * classes ) This function converts multiple existing classes to configurables. Note: This have exactly the same effects of decorate each class with @configurable when defining the class. Each class only need to be decorated once, extra calling of conversion is ignored and has no side effects. Example: import ice import torch.nn as nn ice . make_configurable ( nn . Conv2d , nn . Linear ) assert ice . is_configurable ( nn . Conv2d )","title":"function make_configurable"},{"location":"documents/llutil.config/#function-clone","text":"clone ( obj , deepcopy = True ) clone configurables, containers, and ordinary objects recursively. Args: obj (configurable or list/dict of configurables): the configurable object to be cloned. deepcopy (bool, optional): copy resources by value. Defaults to True. Returns: Unfrozen copy of the original configurable. import ice import torch.nn as nn ice . make_configurable ( nn . Conv2d , nn . Linear ) convcfg = nn . Conv2d ( 16 , 8 ) conv1x1 = convcfg . clone () # or ice.clone(convcfg) conv1x1 [ 'kernel_size' ] = 1 conv1x1 . freeze () # or ice.freeze(conv1x1) assert conv1x1 . kernel_size == ( 1 , 1 ) conv3x3 = convcfg . clone () conv3x3 [ 'kernel_size' ] = 3 conv3x3 . freeze () assert conv3x3 . kernel_size == ( 3 , 3 )","title":"function clone"},{"location":"documents/llutil.config/#function-freeze","text":"freeze ( obj , deepcopy = True ) freeze configurables recursively. Freezing is the process of building the configuration into real objects. Original __init__() functions of configurable classes declared by configurable or make_configurable now will be called recursively to initialize the real instance, also known as the frozen version of a configurable. Args: obj (configurable or list/dict of configurables): the configurable object to be freeze. deepcopy (bool, optional): copy resources by value. Defaults to True. Returns: Frozen version of the original configurable. Note: Freezing happens in-place, ignoring the returned value is safe. If a user wants to reuse the configuration feature, he can clone() the object before or after frozen with the same effect. Example: See examples for configurable and clone .","title":"function freeze"},{"location":"documents/llutil.config/#function-objattr","text":"objattr ( obj , attrname )","title":"function objattr"},{"location":"documents/llutil.config/#class-configurable","text":"","title":"class Configurable"},{"location":"documents/llutil.config/#method-__init__","text":"__init__ ( * args , ** kwds ) \u2192 None","title":"method __init__"},{"location":"documents/llutil.config/#method-clone","text":"clone ( deepcopy = True )","title":"method clone"},{"location":"documents/llutil.config/#method-freeze","text":"freeze ( deepcopy = True )","title":"method freeze"},{"location":"documents/llutil.config/#method-update","text":"update ( explicit = {}, ** implicit )","title":"method update"},{"location":"documents/llutil.dictprocess/","text":"module llutil.dictprocess function dictprocess dictprocess ( f ) a decorator that convert function into a DictProcessor ( Callable[[Dict], Dict] ). ice.dictprocess is a function decorator that convert any function into a callable DictProcessor class that would take a dict as input and update its content. The input arguments and return values of the function are automatically mapped to source and destination the keywords of the state dict being modified. The input arguments mapping rule is simpler. A decorated DictProcessor class can specify fixed parameters at instantiation time, and dynamic parameters as state dict content at runtime. The output arguments mapping is controlled by an extra argument at instantiation time called dst and the return value of the original function, may vary in different scenarios as shown in the following table: dst \\ ret value dict list / tuple None None Do not update, return value directly. Update state dict with returned dict. Do not update, return list / tuple directly. Do nothing. str Update with dict(dst=ret) If len(ret) == 1 , update with dict(dst=ret.values()[0]) ; If dst in ret , update with dict(dst=ret[dst]) ; else update with dict(dst=ret) Update with dict(dst=ret) Update with dict(dst=None) list / tuple Update with {dst[0]:ret} Update with {k:ret[k] for k in dst} Update with {k:v for k, v in zip(dst, ret)} Update with {dst[0]:None} dict(update_key=return_key) Raise TypeError Update with {k:ret[rk] for k, rk in dst.items()} Raise TypeError Raise TypeError Example: import ice @ice . dictprocess def Add ( x , y ): return x + y @ice . dictprocess def Power ( x , n ): return pow ( x , n ) pipeline = [ Add ( x = \"a\" , y = \"b\" , dst = \"c\" ), Power ( x = \"c\" , n = 2 , dst = \"c\" ), ] state_dict = { \"a\" : 1 , \"b\" : 2 } for f in pipeline : state_dict == f ( state_dict ) assert state_dict == { \"a\" : 1 , \"b\" : 2 , \"c\" : 9 } The definition of operations minimizes the boilerplate, and the configuration phase is simple and concise. All these features enables best reusability for complex data processing pipelines. function Compose Compose ( translist : List [ Callable [[ Dict ], Dict ]]) a predefined DictProcessor that composes a list of other DictProcessors together. function Collect Collect ( * keys ) a predefined DictProcessor that keep only selected entries.","title":"Llutil.dictprocess"},{"location":"documents/llutil.dictprocess/#module-llutildictprocess","text":"","title":"module llutil.dictprocess"},{"location":"documents/llutil.dictprocess/#function-dictprocess","text":"dictprocess ( f ) a decorator that convert function into a DictProcessor ( Callable[[Dict], Dict] ). ice.dictprocess is a function decorator that convert any function into a callable DictProcessor class that would take a dict as input and update its content. The input arguments and return values of the function are automatically mapped to source and destination the keywords of the state dict being modified. The input arguments mapping rule is simpler. A decorated DictProcessor class can specify fixed parameters at instantiation time, and dynamic parameters as state dict content at runtime. The output arguments mapping is controlled by an extra argument at instantiation time called dst and the return value of the original function, may vary in different scenarios as shown in the following table: dst \\ ret value dict list / tuple None None Do not update, return value directly. Update state dict with returned dict. Do not update, return list / tuple directly. Do nothing. str Update with dict(dst=ret) If len(ret) == 1 , update with dict(dst=ret.values()[0]) ; If dst in ret , update with dict(dst=ret[dst]) ; else update with dict(dst=ret) Update with dict(dst=ret) Update with dict(dst=None) list / tuple Update with {dst[0]:ret} Update with {k:ret[k] for k in dst} Update with {k:v for k, v in zip(dst, ret)} Update with {dst[0]:None} dict(update_key=return_key) Raise TypeError Update with {k:ret[rk] for k, rk in dst.items()} Raise TypeError Raise TypeError Example: import ice @ice . dictprocess def Add ( x , y ): return x + y @ice . dictprocess def Power ( x , n ): return pow ( x , n ) pipeline = [ Add ( x = \"a\" , y = \"b\" , dst = \"c\" ), Power ( x = \"c\" , n = 2 , dst = \"c\" ), ] state_dict = { \"a\" : 1 , \"b\" : 2 } for f in pipeline : state_dict == f ( state_dict ) assert state_dict == { \"a\" : 1 , \"b\" : 2 , \"c\" : 9 } The definition of operations minimizes the boilerplate, and the configuration phase is simple and concise. All these features enables best reusability for complex data processing pipelines.","title":"function dictprocess"},{"location":"documents/llutil.dictprocess/#function-compose","text":"Compose ( translist : List [ Callable [[ Dict ], Dict ]]) a predefined DictProcessor that composes a list of other DictProcessors together.","title":"function Compose"},{"location":"documents/llutil.dictprocess/#function-collect","text":"Collect ( * keys ) a predefined DictProcessor that keep only selected entries.","title":"function Collect"},{"location":"documents/llutil.launcher/","text":"module llutil.launcher class ElasticLauncher A helper Configurable class for torchrun and torch.distributed.launch . PyTorch's elastic launch ability is embeded in this Configurable, for details please see here . HyperGraph.run() uses this class to launch multiple processes. Directly usage is also possible (see the example below). Example: def worker ( launcher ): print ( \"rank\" , launcher . rank ) print ( \"local_rank\" , launcher . local_rank ) print ( \"device\" , launcher . assigned_device ) if __name__ == \"__main__\" : launcher = ElasticLauncher ( \"cuda:*\" ) . freeze () launcher ( worker , launcher ) method __freeze__ __freeze__ ( devices = 'auto' , nnodes = '1:1' , dist_backend = 'auto' , rdzv_id = 'none' , rdzv_endpoint = '' , rdzv_backend = 'static' , rdzv_configs = '' , standalone = False , max_restarts = 0 , monitor_interval = 5 , start_method = 'spawn' , redirects = '0' , tee = '0' , log_dir = None , role = 'default' , node_rank = 0 , master_addr = '127.0.0.1' , master_port = None , omp_num_threads = 1 ) Args: Worker/node size related arguments: devices (str, optional): enumerates devices on this node, e.g.: [ \"auto\" , \"cpu\" , \"cuda\" , \"cuda:0\" , \"cuda:*\" , \"auto:*\" , \"cuda:1,3\" , \"cuda:0-2,7\" ]. Defaults to \"auto\" . dist_backend (str, optional): supports: [ \"nccl\" , \"gloo\" , \"mpi\" , \"auto\" ]. If given \"auto\" , will use \"nccl\" for \"cuda\" and \"gloo\" for \"cpu\" in general. Defaults to \"auto\" . nnodes (str, optional): Number of nodes, or the range of nodes in form <minimum_nodes>:<maximum_nodes> . Defaults to \"1:1\" . Rendezvous related arguments: rdzv_id (str, optional): User-defined group id. rdzv_endpoint (str, optional): Rendezvous backend endpoint; usually in form <host>:<port> . rdzv_backend (str, optional): Rendezvous backend. rdzv_configs (str, optional): Additional rendezvous configuration ( <key1>=<value1>,<key2>=<value2>,... ). standalone (bool, optional): Start a local standalone rendezvous backend that is represented by a C10d TCP store on port 29400. Useful when launching single-node, multi-worker job. If specified rdzv_backend, rdzv_endpoint, rdzv_id are auto-assigned; any explicitly set values are ignored. Defaults to False . User-code launch related arguments: max_restarts (int, optional): Maximum number of worker group restarts before failing. Defaults to 0. monitor_interval (int, optional): Interval, in seconds, to monitor the state of workers. Defaults to 5. start_method (str, optional): Multiprocessing start method to use when creating workers. Defaults to \"spawn\" . redirects (str, optional): Redirect std streams into a log file in the log directory (e.g. 3 redirects both stdout+stderr for all workers, 0:1,1:2 redirects stdout for local rank 0 and stderr for local rank 1). Defaults to \"0\" . tee (str, optional): Tee std streams into a log file and also to console (see redirects for format). Defaults to \"0\" . log_dir ([type], optional): Base directory to use for log files (e.g. /var/log/torch/elastic). The same directory is re-used for multiple runs (a unique job-level sub-directory is created with rdzv_id as the prefix). Defaults to None. role (str, optional): User-defined role for the workers. Defaults to \"default\" . Backwards compatible parameters with caffe2.distributed.launch : node_rank (int, optional): \"Rank of the node for multi-node distributed training.\"). Defaults to 0. master_addr (str, optional): Address of the master node (rank 0). It should be either the IP address or the hostname of rank 0. For single node multi-proc training the master_addr can simply be 127.0.0.1; IPv6 should have the pattern [0:0:0:0:0:0:0:1] .\") Defaults to \"127.0.0.1\". master_port ([type], optional): Port on the master node (rank 0) to be used for communication during distributed training. Defaults will generate a random port between 16894 and 17194 . omp_num_threads (int, optional): set OMP_NUM_THREADS environment if not exists. Defaults to 1. property assigned_device property devices property dist_backend property group_rank property group_world_size property local_rank property local_world_size property master_addr property master_port property max_restarts property rank property rdzv_id property restart_count property role_name property role_rank property role_world_size property world_size","title":"Llutil.launcher"},{"location":"documents/llutil.launcher/#module-llutillauncher","text":"","title":"module llutil.launcher"},{"location":"documents/llutil.launcher/#class-elasticlauncher","text":"A helper Configurable class for torchrun and torch.distributed.launch . PyTorch's elastic launch ability is embeded in this Configurable, for details please see here . HyperGraph.run() uses this class to launch multiple processes. Directly usage is also possible (see the example below). Example: def worker ( launcher ): print ( \"rank\" , launcher . rank ) print ( \"local_rank\" , launcher . local_rank ) print ( \"device\" , launcher . assigned_device ) if __name__ == \"__main__\" : launcher = ElasticLauncher ( \"cuda:*\" ) . freeze () launcher ( worker , launcher )","title":"class ElasticLauncher"},{"location":"documents/llutil.launcher/#method-__freeze__","text":"__freeze__ ( devices = 'auto' , nnodes = '1:1' , dist_backend = 'auto' , rdzv_id = 'none' , rdzv_endpoint = '' , rdzv_backend = 'static' , rdzv_configs = '' , standalone = False , max_restarts = 0 , monitor_interval = 5 , start_method = 'spawn' , redirects = '0' , tee = '0' , log_dir = None , role = 'default' , node_rank = 0 , master_addr = '127.0.0.1' , master_port = None , omp_num_threads = 1 ) Args: Worker/node size related arguments: devices (str, optional): enumerates devices on this node, e.g.: [ \"auto\" , \"cpu\" , \"cuda\" , \"cuda:0\" , \"cuda:*\" , \"auto:*\" , \"cuda:1,3\" , \"cuda:0-2,7\" ]. Defaults to \"auto\" . dist_backend (str, optional): supports: [ \"nccl\" , \"gloo\" , \"mpi\" , \"auto\" ]. If given \"auto\" , will use \"nccl\" for \"cuda\" and \"gloo\" for \"cpu\" in general. Defaults to \"auto\" . nnodes (str, optional): Number of nodes, or the range of nodes in form <minimum_nodes>:<maximum_nodes> . Defaults to \"1:1\" . Rendezvous related arguments: rdzv_id (str, optional): User-defined group id. rdzv_endpoint (str, optional): Rendezvous backend endpoint; usually in form <host>:<port> . rdzv_backend (str, optional): Rendezvous backend. rdzv_configs (str, optional): Additional rendezvous configuration ( <key1>=<value1>,<key2>=<value2>,... ). standalone (bool, optional): Start a local standalone rendezvous backend that is represented by a C10d TCP store on port 29400. Useful when launching single-node, multi-worker job. If specified rdzv_backend, rdzv_endpoint, rdzv_id are auto-assigned; any explicitly set values are ignored. Defaults to False . User-code launch related arguments: max_restarts (int, optional): Maximum number of worker group restarts before failing. Defaults to 0. monitor_interval (int, optional): Interval, in seconds, to monitor the state of workers. Defaults to 5. start_method (str, optional): Multiprocessing start method to use when creating workers. Defaults to \"spawn\" . redirects (str, optional): Redirect std streams into a log file in the log directory (e.g. 3 redirects both stdout+stderr for all workers, 0:1,1:2 redirects stdout for local rank 0 and stderr for local rank 1). Defaults to \"0\" . tee (str, optional): Tee std streams into a log file and also to console (see redirects for format). Defaults to \"0\" . log_dir ([type], optional): Base directory to use for log files (e.g. /var/log/torch/elastic). The same directory is re-used for multiple runs (a unique job-level sub-directory is created with rdzv_id as the prefix). Defaults to None. role (str, optional): User-defined role for the workers. Defaults to \"default\" . Backwards compatible parameters with caffe2.distributed.launch : node_rank (int, optional): \"Rank of the node for multi-node distributed training.\"). Defaults to 0. master_addr (str, optional): Address of the master node (rank 0). It should be either the IP address or the hostname of rank 0. For single node multi-proc training the master_addr can simply be 127.0.0.1; IPv6 should have the pattern [0:0:0:0:0:0:0:1] .\") Defaults to \"127.0.0.1\". master_port ([type], optional): Port on the master node (rank 0) to be used for communication during distributed training. Defaults will generate a random port between 16894 and 17194 . omp_num_threads (int, optional): set OMP_NUM_THREADS environment if not exists. Defaults to 1.","title":"method __freeze__"},{"location":"documents/llutil.launcher/#property-assigned_device","text":"","title":"property assigned_device"},{"location":"documents/llutil.launcher/#property-devices","text":"","title":"property devices"},{"location":"documents/llutil.launcher/#property-dist_backend","text":"","title":"property dist_backend"},{"location":"documents/llutil.launcher/#property-group_rank","text":"","title":"property group_rank"},{"location":"documents/llutil.launcher/#property-group_world_size","text":"","title":"property group_world_size"},{"location":"documents/llutil.launcher/#property-local_rank","text":"","title":"property local_rank"},{"location":"documents/llutil.launcher/#property-local_world_size","text":"","title":"property local_world_size"},{"location":"documents/llutil.launcher/#property-master_addr","text":"","title":"property master_addr"},{"location":"documents/llutil.launcher/#property-master_port","text":"","title":"property master_port"},{"location":"documents/llutil.launcher/#property-max_restarts","text":"","title":"property max_restarts"},{"location":"documents/llutil.launcher/#property-rank","text":"","title":"property rank"},{"location":"documents/llutil.launcher/#property-rdzv_id","text":"","title":"property rdzv_id"},{"location":"documents/llutil.launcher/#property-restart_count","text":"","title":"property restart_count"},{"location":"documents/llutil.launcher/#property-role_name","text":"","title":"property role_name"},{"location":"documents/llutil.launcher/#property-role_rank","text":"","title":"property role_rank"},{"location":"documents/llutil.launcher/#property-role_world_size","text":"","title":"property role_world_size"},{"location":"documents/llutil.launcher/#property-world_size","text":"","title":"property world_size"},{"location":"documents/llutil.logging/","text":"module llutil.logging logging utilities. function get_logger get_logger ( name : Optional [ str ] = None ) set up a simple logger that writes into stderr. The loglevel is fetched from the LOGLEVEL env. variable or WARNING as default. The function will use the module name of the caller if no name is provided. Args: name : Name of the logger. If no name provided, the name will be derived from the call stack.","title":"Llutil.logging"},{"location":"documents/llutil.logging/#module-llutillogging","text":"logging utilities.","title":"module llutil.logging"},{"location":"documents/llutil.logging/#function-get_logger","text":"get_logger ( name : Optional [ str ] = None ) set up a simple logger that writes into stderr. The loglevel is fetched from the LOGLEVEL env. variable or WARNING as default. The function will use the module name of the caller if no name is provided. Args: name : Name of the logger. If no name provided, the name will be derived from the call stack.","title":"function get_logger"},{"location":"documents/llutil/","text":"module llutil implements low-level utilities.","title":"Llutil"},{"location":"documents/llutil/#module-llutil","text":"implements low-level utilities.","title":"module llutil"},{"location":"documents/llutil.multiprocessing/","text":"module llutil.multiprocessing a drop-in replacement for torch.multiprocessing . ice.llutil.multiprocessing is a modified version of torch.multiprocessing . It's designed to change import torch.multiprocessing as mp to from ice import multiprocessing as mp to have all the lambda functions, closures as well as pytorch tensors sent through processes in Data Distributed Parallel paradigm. Because of the similarity of APIs we do not document most of this package contents, and we recommend referring to very good docs of the original module. Global Variables reductions function called_from_main called_from_main () Another version of if __name__ == \"__main__\" that works everywhere. Returns: bool","title":"Llutil.multiprocessing"},{"location":"documents/llutil.multiprocessing/#module-llutilmultiprocessing","text":"a drop-in replacement for torch.multiprocessing . ice.llutil.multiprocessing is a modified version of torch.multiprocessing . It's designed to change import torch.multiprocessing as mp to from ice import multiprocessing as mp to have all the lambda functions, closures as well as pytorch tensors sent through processes in Data Distributed Parallel paradigm. Because of the similarity of APIs we do not document most of this package contents, and we recommend referring to very good docs of the original module.","title":"module llutil.multiprocessing"},{"location":"documents/llutil.multiprocessing/#global-variables","text":"reductions","title":"Global Variables"},{"location":"documents/llutil.multiprocessing/#function-called_from_main","text":"called_from_main () Another version of if __name__ == \"__main__\" that works everywhere. Returns: bool","title":"function called_from_main"},{"location":"documents/llutil.pycuda/","text":"module llutil.pycuda Integrates PyCUDA to PyTorch and ice. class CUDAModule Just-In-Time compilation of a set of CUDA kernel functions and device functions from source. ice.CUDAModule works differently compared to pycuda's SourceModule in following ways: Support efficient multi-dimensional torch.Tensor access with optional boundary check. Automatically handle scalar data type conversion from python/pytorch to c++. Compile error message will report the original source code. Easier API. Configurable in the ice-learn eco-system. Example: import ice M , N , K = 4 , 4 , 1 a = torch . rand (( M , K ), dtype = torch . float32 ) . cuda () b = torch . rand (( K , N ), dtype = torch . float32 ) . cuda () c = torch . empty (( M , N ), dtype = torch . float32 ) . cuda () kernels = ice . CUDAModule ( r \"\"\" __global__ void matmul(Tensor<float, 2> *a, Tensor<float, 2> *b, Tensor<float, 2> *c, int M, int N, int K) { int m = blockIdx.y * blockDim.y + threadIdx.y; int n = blockIdx.x * blockDim.x + threadIdx.x; float v = 0.f; if (m >= M || n >= N) return; for (int k = 0; k < K; ++k) { v += (*a)[m][k] * (*b)[k][n]; } (*c)[m][n] = v; } \"\"\" , float_bits = 32 ) . freeze () kernels . matmul ( a , b , c , M , N , K , grid = ( N // 32 + 1 , M // 32 + 1 ), block = ( 32 , 32 , 1 )) torch . cuda . synchronize () assert torch . allclose ( c , torch . mm ( a , b )) method __freeze__ __freeze__ ( source , float_bits , int_bits = 32 , include_dirs = [], boundscheck = True , ** kwds ) Setup the parameters for compiling a CUDA source. Args: source (str): CUDA C++ source string. float_bits (int): bit width of float values used as tensor scalar. int_bits (int, optional): bit width of default int scalar. Defaults to 32. include_dirs (list, optional): paths of extra include dirs. Defaults to []. boundscheck (bool, optional): enable out of bound check for tensors. Defaults to True. **kwds : other keyword args you would like to pass to pycuda's SourceModule . Note: Direct written float and int token in the source string will be substituted to ensure the default scalar data type matches the tensors. If you do not want this to happen, use more specific CUDA typename such as __half , double , int16_t , etc.","title":"Llutil.pycuda"},{"location":"documents/llutil.pycuda/#module-llutilpycuda","text":"Integrates PyCUDA to PyTorch and ice.","title":"module llutil.pycuda"},{"location":"documents/llutil.pycuda/#class-cudamodule","text":"Just-In-Time compilation of a set of CUDA kernel functions and device functions from source. ice.CUDAModule works differently compared to pycuda's SourceModule in following ways: Support efficient multi-dimensional torch.Tensor access with optional boundary check. Automatically handle scalar data type conversion from python/pytorch to c++. Compile error message will report the original source code. Easier API. Configurable in the ice-learn eco-system. Example: import ice M , N , K = 4 , 4 , 1 a = torch . rand (( M , K ), dtype = torch . float32 ) . cuda () b = torch . rand (( K , N ), dtype = torch . float32 ) . cuda () c = torch . empty (( M , N ), dtype = torch . float32 ) . cuda () kernels = ice . CUDAModule ( r \"\"\" __global__ void matmul(Tensor<float, 2> *a, Tensor<float, 2> *b, Tensor<float, 2> *c, int M, int N, int K) { int m = blockIdx.y * blockDim.y + threadIdx.y; int n = blockIdx.x * blockDim.x + threadIdx.x; float v = 0.f; if (m >= M || n >= N) return; for (int k = 0; k < K; ++k) { v += (*a)[m][k] * (*b)[k][n]; } (*c)[m][n] = v; } \"\"\" , float_bits = 32 ) . freeze () kernels . matmul ( a , b , c , M , N , K , grid = ( N // 32 + 1 , M // 32 + 1 ), block = ( 32 , 32 , 1 )) torch . cuda . synchronize () assert torch . allclose ( c , torch . mm ( a , b ))","title":"class CUDAModule"},{"location":"documents/llutil.pycuda/#method-__freeze__","text":"__freeze__ ( source , float_bits , int_bits = 32 , include_dirs = [], boundscheck = True , ** kwds ) Setup the parameters for compiling a CUDA source. Args: source (str): CUDA C++ source string. float_bits (int): bit width of float values used as tensor scalar. int_bits (int, optional): bit width of default int scalar. Defaults to 32. include_dirs (list, optional): paths of extra include dirs. Defaults to []. boundscheck (bool, optional): enable out of bound check for tensors. Defaults to True. **kwds : other keyword args you would like to pass to pycuda's SourceModule . Note: Direct written float and int token in the source string will be substituted to ensure the default scalar data type matches the tensors. If you do not want this to happen, use more specific CUDA typename such as __half , double , int16_t , etc.","title":"method __freeze__"},{"location":"documents/llutil.test/","text":"module llutil.test helps developers of ice to test. function requires_n_gpus requires_n_gpus ( n )","title":"Llutil.test"},{"location":"documents/llutil.test/#module-llutiltest","text":"helps developers of ice to test.","title":"module llutil.test"},{"location":"documents/llutil.test/#function-requires_n_gpus","text":"requires_n_gpus ( n )","title":"function requires_n_gpus"},{"location":"resources/dev_notes/00_setup_devenv/","text":"Setup Environment We use poetry as the virtualenv as well as project manager (e.g. dependencies, packaging, publishing, etc.). Please read about poetry (see REFERNCES section at the end of this page) and the pyproject.toml file before you run following commands to setup your local develop environment. Having another pip installed release version of ice will not cause a problem since poetry isolates the environment. Pypi Mirror for China (Optional) mkdir -p ~/.pip echo \"[config]\\nindex-url = https://pypi.douban.com/simple\" > ~/.pip/pip.conf Steps Install poetry following the instruction here . Set tab-completion for poetry following the instruction here git clone https://github.com/tjyuyao/ice-learn cd ice-learn poetry shell poetry install -E pycuda Set tab-completion for poe-the-poet following the instruction here . Install torch and torchvision manually for correct cuda version using pip in the poetry shell, e.g.: pip3 install torch==1.10.1+cu113 torchvision==0.11.2+cu113 torchaudio==0.10.1+cu113 -f https://download.pytorch.org/whl/cu113/torch_stable.html References Poetry First Impression Poetry Tutorial Poetry API Reference","title":"Setup Environment"},{"location":"resources/dev_notes/00_setup_devenv/#setup-environment","text":"We use poetry as the virtualenv as well as project manager (e.g. dependencies, packaging, publishing, etc.). Please read about poetry (see REFERNCES section at the end of this page) and the pyproject.toml file before you run following commands to setup your local develop environment. Having another pip installed release version of ice will not cause a problem since poetry isolates the environment.","title":"Setup Environment"},{"location":"resources/dev_notes/00_setup_devenv/#pypi-mirror-for-china-optional","text":"mkdir -p ~/.pip echo \"[config]\\nindex-url = https://pypi.douban.com/simple\" > ~/.pip/pip.conf","title":"Pypi Mirror for China (Optional)"},{"location":"resources/dev_notes/00_setup_devenv/#steps","text":"Install poetry following the instruction here . Set tab-completion for poetry following the instruction here git clone https://github.com/tjyuyao/ice-learn cd ice-learn poetry shell poetry install -E pycuda Set tab-completion for poe-the-poet following the instruction here . Install torch and torchvision manually for correct cuda version using pip in the poetry shell, e.g.: pip3 install torch==1.10.1+cu113 torchvision==0.11.2+cu113 torchaudio==0.10.1+cu113 -f https://download.pytorch.org/whl/cu113/torch_stable.html","title":"Steps"},{"location":"resources/dev_notes/00_setup_devenv/#references","text":"Poetry First Impression Poetry Tutorial Poetry API Reference","title":"References"},{"location":"resources/dev_notes/01_contribution_guide/","text":"Contribution Guide Authors fork and submit pull requests. The project owner will review and merge them. Changes does not need to be rebased, you can keep every history. But try to follow below constraints for commit messages: Prefix it with at least one of these words: [Tiny] for trivial modification. [Deps] when commits contain dependency variation. [Bugs] fixed ... when you fixed one or more bugs. [Bugs] tbfix ... when you discovered bugs but not jet fixed. Add TODO tags and descriptions in the code comments. [Feat] when a new feature is basically/fully implemented, or behavior changed. [Docs] when only documentation is modified. Note that you should modify tutorials and devnotes in docs folder directly, but modify references in source code docstring following Google docstring style, as it is automatically generated using lazydocs . [Misc] for other cases. After the prefix, describe the details with concise but meaningful words. Commits can be both small and large, but try to always accomplish one concise and meaningful thing.","title":"Contribution Guide"},{"location":"resources/dev_notes/01_contribution_guide/#contribution-guide","text":"Authors fork and submit pull requests. The project owner will review and merge them. Changes does not need to be rebased, you can keep every history. But try to follow below constraints for commit messages: Prefix it with at least one of these words: [Tiny] for trivial modification. [Deps] when commits contain dependency variation. [Bugs] fixed ... when you fixed one or more bugs. [Bugs] tbfix ... when you discovered bugs but not jet fixed. Add TODO tags and descriptions in the code comments. [Feat] when a new feature is basically/fully implemented, or behavior changed. [Docs] when only documentation is modified. Note that you should modify tutorials and devnotes in docs folder directly, but modify references in source code docstring following Google docstring style, as it is automatically generated using lazydocs . [Misc] for other cases. After the prefix, describe the details with concise but meaningful words. Commits can be both small and large, but try to always accomplish one concise and meaningful thing.","title":"Contribution Guide"},{"location":"resources/dev_notes/02_docs_and_tests/","text":"Documents & Tests This page describes solutions ice-learn adopts about documentation generation, unit tests and documentation-based test. Note that following commands should be executed in a poetry shell , or prepended by poetry run . pytest We use pytest for unittesting. For example, pytest tests/llutil/test_config.py -vs where -s will print outputs for failed tests. Run specific tests with test_mod.py::TestClass:test_method or test_mod.py::test_function . pytest tests/llutil/test_multiprocessing.py::test_reduce -v Option -m slow will run tests decorated with the @pytest.mark.slow decorator. pytest tests/llutil/test_multiprocessing.py -v -m slow Option -m \"not slow\" will run tests not decorated with the @pytest.mark.slow decorator. pytest tests/llutil/ -v -m \"not slow\" See more configurations in pytest.ini . ice.llutil.test has some extensions for conditional tests for conveniency. xdoctest We use xdoctest for testing demostration code in docstring. For example, python -m xdoctest ice/llutil/config.py or pytest ice/llutil/ -v --xdoc lazydocs We generate markdown documentations from docstring using a modified lazydocs script. For example, python ./docs/build_docs.py mkdocs We generate the documents site from manually written as well as autogenerated markdown using mkdocs , the plugin mkdocs-awesome-pages and the theme mkdocs-material . In the ./docs directory, files in documents are automatically generated by lazydocs , while other subfolders contain manually written docs. Configuration file is mkdocs.yml . Changing the navigator appearance of the document site requires modifying .pages file in each level of subdirectories. Run following command locally to preview the site at the root directory of ice-learn project: mkdocs serve Run following command to push documents to gh-pages branch (for Github Pages). mkdocs gh-deploy See this page for more infomation on deploying documents.","title":"Documents & Tests"},{"location":"resources/dev_notes/02_docs_and_tests/#documents-tests","text":"This page describes solutions ice-learn adopts about documentation generation, unit tests and documentation-based test. Note that following commands should be executed in a poetry shell , or prepended by poetry run .","title":"Documents &amp; Tests"},{"location":"resources/dev_notes/02_docs_and_tests/#pytest","text":"We use pytest for unittesting. For example, pytest tests/llutil/test_config.py -vs where -s will print outputs for failed tests. Run specific tests with test_mod.py::TestClass:test_method or test_mod.py::test_function . pytest tests/llutil/test_multiprocessing.py::test_reduce -v Option -m slow will run tests decorated with the @pytest.mark.slow decorator. pytest tests/llutil/test_multiprocessing.py -v -m slow Option -m \"not slow\" will run tests not decorated with the @pytest.mark.slow decorator. pytest tests/llutil/ -v -m \"not slow\" See more configurations in pytest.ini . ice.llutil.test has some extensions for conditional tests for conveniency.","title":"pytest"},{"location":"resources/dev_notes/02_docs_and_tests/#xdoctest","text":"We use xdoctest for testing demostration code in docstring. For example, python -m xdoctest ice/llutil/config.py or pytest ice/llutil/ -v --xdoc","title":"xdoctest"},{"location":"resources/dev_notes/02_docs_and_tests/#lazydocs","text":"We generate markdown documentations from docstring using a modified lazydocs script. For example, python ./docs/build_docs.py","title":"lazydocs"},{"location":"resources/dev_notes/02_docs_and_tests/#mkdocs","text":"We generate the documents site from manually written as well as autogenerated markdown using mkdocs , the plugin mkdocs-awesome-pages and the theme mkdocs-material . In the ./docs directory, files in documents are automatically generated by lazydocs , while other subfolders contain manually written docs. Configuration file is mkdocs.yml . Changing the navigator appearance of the document site requires modifying .pages file in each level of subdirectories. Run following command locally to preview the site at the root directory of ice-learn project: mkdocs serve Run following command to push documents to gh-pages branch (for Github Pages). mkdocs gh-deploy See this page for more infomation on deploying documents.","title":"mkdocs"},{"location":"resources/dev_notes/03_architechture_design/","text":"The Folder Structure llutil for Low-Level Utilities that supports the most fundamental mechanisms. multiprocessing enables cross-processes communication for tensors, lambdas and closures. config converts every class into a configurable . modifier defines a tool for pipeline operation, useful in dataset transform and learning rate update, etc. core implements the kernel hyper-graph architecture for scheduling multitasks and experiments. api organizes all resources and provide user-friendly interfaces. repro reproduces public works using low-level or high-level APIs in ice . Models, tricks, and other useful code will be incorporated back into api if in need.","title":"The Folder Structure"},{"location":"resources/dev_notes/03_architechture_design/#the-folder-structure","text":"llutil for Low-Level Utilities that supports the most fundamental mechanisms. multiprocessing enables cross-processes communication for tensors, lambdas and closures. config converts every class into a configurable . modifier defines a tool for pipeline operation, useful in dataset transform and learning rate update, etc. core implements the kernel hyper-graph architecture for scheduling multitasks and experiments. api organizes all resources and provide user-friendly interfaces. repro reproduces public works using low-level or high-level APIs in ice . Models, tricks, and other useful code will be incorporated back into api if in need.","title":"The Folder Structure"},{"location":"user_guide/","text":"Tutorials","title":"Tutorials"},{"location":"user_guide/#tutorials","text":"","title":"Tutorials"},{"location":"user_guide/01_debug_tips/","text":"Common tips for debugging ICE programs Debug by print Debug by breakpoint set device to cuda or cpu at first debugging ddp: if torch . cuda . current_device () == 0 : breakpoint ()","title":"Common tips for debugging ICE programs"},{"location":"user_guide/01_debug_tips/#common-tips-for-debugging-ice-programs","text":"","title":"Common tips for debugging ICE programs"},{"location":"user_guide/01_debug_tips/#debug-by-print","text":"","title":"Debug by print"},{"location":"user_guide/01_debug_tips/#debug-by-breakpoint","text":"set device to cuda or cpu at first debugging ddp: if torch . cuda . current_device () == 0 : breakpoint ()","title":"Debug by breakpoint"}]}